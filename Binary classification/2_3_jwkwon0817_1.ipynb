{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_3_jwkwon0817_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nidOVmMCwp0N"
      },
      "source": [
        "# **Mount**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOac6vMUwrH4",
        "outputId": "279ee1e9-937c-4df9-dfc1-e7929f3ae9be"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo_nLizUw1h9"
      },
      "source": [
        "# **Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0l-UahFw2RI"
      },
      "source": [
        "MAX_ROUNDS = 400\n",
        "OPTIMIZE_ROUNDS = False\n",
        "LEARNING_RATE = 0.07\n",
        "EARLY_STOPPING_ROUNDS = 50\n",
        "# Note: I set EARLY_STOPPING_ROUNDS high so that (when OPTIMIZE_ROUNDS is set)\n",
        "# I will get lots of information to make my own judgement. You should probably\n",
        "# reduce EARLY_STOPPING_ROUNDS if you want to do actual early stopping."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Glg573xOq4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from numba import jit\n",
        "import time\n",
        "import gc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YryxLNZPxjSr"
      },
      "source": [
        "### **Compute gini**\n",
        "### **_[from CPMP's kernel](https://www.kaggle.com/cpmpml/extremely-fast-gini-computation)_**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0KrqtUwxoWS"
      },
      "source": [
        "@jit\n",
        "def eval_gini(y_true, y_prob):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_true = y_true[np.argsort(y_prob)]\n",
        "    ntrue = 0\n",
        "    gini = 0\n",
        "    delta = 0\n",
        "    n = len(y_true)\n",
        "    for i in range(n-1, -1, -1):\n",
        "        y_i = y_true[i]\n",
        "        ntrue += y_i\n",
        "        gini += y_i * delta\n",
        "        delta += 1 - y_i\n",
        "    gini = 1 - 2 * gini / (ntrue*(n-ntrue))\n",
        "    return gini"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7md6rnpgyMrC"
      },
      "source": [
        "### **_[Functions from olivier's kernel](https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283)_**\n",
        "\n",
        "### **_[Smoothing is computed like in the following paper by Daniele Micci-Barreca](https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf)_**\n",
        "\n",
        "* **trn_series: training categorical feature as a pd.Series**\n",
        "* **tst_series: test categorical feature as a pd.Series**\n",
        "* **target: target data as a pd.Series**\n",
        "* **min_samples_leaf (int): minimum samples to take category average into account**\n",
        "* **smoothing (int): smoothing effect to balance categorical average vs prior**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPZuX_DiyZye"
      },
      "source": [
        "def gini_xgb(preds, dtrain):\n",
        "    labels = dtrain.get_label()\n",
        "    gini_score = -eval_gini(labels, preds)\n",
        "    return [('gini', gini_score)]\n",
        "\n",
        "def add_noise(series, noise_level):\n",
        "    return series * (1+noise_level*np.random.randn(len(series)))\n",
        "\n",
        "def target_encode(trn_series=None, # Revised to encode validation series\n",
        "                  val_series=None, \n",
        "                  tst_series=None, \n",
        "                  target=None, \n",
        "                  min_samples_leaf=1, \n",
        "                  smoothing=1, \n",
        "                  noise_level=0):\n",
        "    \n",
        "    assert len(trn_series) == len(target)\n",
        "    assert trn_series.name == tst_series.name\n",
        "    temp = pd.concat([trn_series, target], axis=1)\n",
        "\n",
        "    # Compute target mean\n",
        "    averages = temp.groupby(by=trn_series.name)[target.name].agg(['mean', 'count'])\n",
        "\n",
        "    # Compute smoothing\n",
        "    smoothing = 1 / (1+np.exp(-(averages['count']-min_samples_leaf)/smoothing))\n",
        "\n",
        "    # Apply average function to all target data\n",
        "    prior = target.mean()\n",
        "\n",
        "    # The bigger the count the less full_avg is taken into account\n",
        "    averages[target.name] = prior * (1-smoothing) + averages['mean'] * smoothing\n",
        "    averages.drop(['mean', 'count'], axis=1, inplace=True)\n",
        "\n",
        "    # Apply averages to trn and tst series\n",
        "    ft_trn_series = pd.merge(\n",
        "        trn_series.to_frame(trn_series.name), \n",
        "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}), \n",
        "        on=trn_series.name, \n",
        "        how='left')['average'].rename(trn_series.name+'_mean').fillna(prior)\n",
        "\n",
        "    # pd.merge does not keep the index so restore it\n",
        "    ft_trn_series.index = trn_series.index\n",
        "    ft_val_series = pd.merge(\n",
        "        val_series.to_frame(val_series.name), \n",
        "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}), \n",
        "        on=val_series.name, \n",
        "        how='left')['average'].rename(trn_series.name+'_mean').fillna(prior)\n",
        "\n",
        "    # pd.merge does not keep the index so restore it\n",
        "    ft_val_series.index = val_series.index\n",
        "    ft_tst_series = pd.merge(\n",
        "        tst_series.to_frame(tst_series.name), \n",
        "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}), \n",
        "        on=tst_series.name, \n",
        "        how='left')['average'].rename(trn_series.name+'_mean').fillna(prior)\n",
        "\n",
        "    # pd.merge does not keep the index so restore it\n",
        "    ft_tst_series.index = tst_series.index\n",
        "    return add_noise(ft_trn_series, noise_level), \\\n",
        "    add_noise(ft_val_series, noise_level), \\\n",
        "    add_noise(ft_tst_series, noise_level)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nWE4gGR1WIt"
      },
      "source": [
        "# Read data\n",
        "train_df = pd.read_csv('drive/MyDrive/input_2/train.csv', na_values='-1') # .iloc[0:200, :]\n",
        "test_df = pd.read_csv('drive/MyDrive/input_2/test.csv', na_values='-1')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMa4yQpr14WJ"
      },
      "source": [
        "# from olivier\n",
        "train_features = [\n",
        "                  'ps_car_13', # : 1571.65 / shadow 609.23\n",
        "                  'ps_reg_03', # : 1408.42 / shadow 511.15\n",
        "                  'ps_ind_05_cat', # : 1387.87 / shadow 84.72\n",
        "                  'ps_ind_03', # : 1219.47 / shadow 230.55\n",
        "                  'ps_ind_15', # : 922.18 / shadow 242.00\n",
        "                  'ps_reg_02', # : 920.65 / shadow 267.50\n",
        "                  'ps_car_14', # : 798.48 / shadow 549.58\n",
        "                  'ps_car_12', # : 731.93 / shadow 293.62\n",
        "                  'ps_car_01_cat', # : 698.07 / shadow 178.72\n",
        "                  'ps_car_07_cat', # : 694.53 / shadow 36.35\n",
        "                  'ps_ind_17_bin', # : 620.77 / shadow 23.15\n",
        "                  'ps_car_03_cat', # : 611.73 / shadow 50.67\n",
        "                  'ps_reg_01', # : 598.60 / shadow 178.57\n",
        "                  'ps_car_15', # : 593.35 / shadow 226.43\n",
        "                  'ps_ind_01', # : 547.32 / shadow 154.58\n",
        "                  'ps_ind_16_bin', # : 475.37 / shadow 34.17\n",
        "                  'ps_ind_07_bin', # : 435.28 / shadow 28.92\n",
        "                  'ps_car_06_cat', # : 398.02 / shadow 212.43\n",
        "                  'ps_car_04_cat', # : 376.87 / shadow 76.98\n",
        "                  'ps_ind_06_bin', # : 370.97 / shadow 36.13\n",
        "                  'ps_car_09_cat', # : 214.12 / shadow 81.38\n",
        "                  'ps_car_02_cat', # : 203.03 / shadow 26.67\n",
        "                  'ps_ind_02_cat', # : 189.47 / shadow 65.68\n",
        "                  'ps_car_11', # : 173.28 / shadow 76.45\n",
        "                  'ps_car_05_cat', # : 172.75 / shadow 62.92\n",
        "                  'ps_calc_09', # : 169.13 / shadow 129.72\n",
        "                  'ps_calc_05', # : 148.83 / shadow 120.68\n",
        "                  'ps_ind_08_bin', # : 140.73 / shadow 27.63\n",
        "                  'ps_car_08_cat', # : 120.87 / shadow 28.82\n",
        "                  'ps_ind_09_bin', # : 113.92 / shadow 27.05\n",
        "                  'ps_ind_04_cat', # : 107.27 / shadow 37.43\n",
        "                  'ps_ind_18_bin', # : 77.42 / shadow 25.97\n",
        "                  'ps_ind_12_bin', # : 39.67 / shadow 15.52\n",
        "                  'ps_ind_14' # 37.37 / shadow 16.65\n",
        "]\n",
        "\n",
        "# add combinations\n",
        "combs = [\n",
        "         ('ps_reg_01', 'ps_car_02_cat'), \n",
        "         ('ps_reg_01', 'ps_car_04_cat')\n",
        "]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRVTbWQL4wRX",
        "outputId": "a1143ac5-3996-4f14-d4c0-431125127e46"
      },
      "source": [
        "# Process data\n",
        "id_test = test_df['id'].values\n",
        "id_train = train_df['id'].values\n",
        "y = train_df['target']\n",
        "\n",
        "start = time.time()\n",
        "for n_c, (f1, f2) in enumerate(combs):\n",
        "    name1 = f1 + '_plus_' + f2\n",
        "    print('current feature %60s %4d in %5.1f' % (name1, n_c+1, (time.time()-start)/60), end='')\n",
        "    print('\\r' * 75, end='')\n",
        "    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + '_' + train_df[f2].apply(lambda x: str(x))\n",
        "    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + '_' + test_df[f2].apply(lambda x: str(x))\n",
        "    \n",
        "    # Label Encode\n",
        "    lbl = LabelEncoder()\n",
        "    lbl.fit(list(train_df[name1].values)+list(test_df[name1].values))\n",
        "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
        "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
        "\n",
        "    train_features.append(name1)\n",
        "\n",
        "X = train_df[train_features]\n",
        "test_df = test_df[train_features]\n",
        "\n",
        "f_cats = [f for f in X.columns if '_cat' in f]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz0DMBIb5_nS"
      },
      "source": [
        "y_valid_pred = 0 * y\n",
        "y_test_pred = 0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOinl7lT6EUf"
      },
      "source": [
        "# Set up folds\n",
        "K = 5\n",
        "kf = KFold(n_splits=K, random_state=1, shuffle=True)\n",
        "np.random.seed(0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ_sycmp6LfV"
      },
      "source": [
        "# Set up classifier\n",
        "model = XGBClassifier(\n",
        "    n_estimators=MAX_ROUNDS, \n",
        "    max_depth=4, \n",
        "    objective='binary:logistic', \n",
        "    learning_rate=LEARNING_RATE, \n",
        "    subsample=.8, \n",
        "    min_child_weight=6, \n",
        "    colsample_bytree=.8, \n",
        "    scale_pos_weight=1.6, \n",
        "    gamma=10, \n",
        "    reg_alpha=8, \n",
        "    reg_lambda=1.3\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAp7alqR6uxd",
        "outputId": "c7ae8039-fbac-4a52-8c2a-a72ef62e5134"
      },
      "source": [
        "# Run CV\n",
        "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
        "    # Create data for this fold\n",
        "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
        "    X_train, X_valid = X.iloc[train_index, :].copy(), X.iloc[test_index, :].copy()\n",
        "    X_test = test_df.copy()\n",
        "    print('\\nFold', i)\n",
        "\n",
        "    # Encoded data\n",
        "    for f in f_cats:\n",
        "        X_train[f+'_avg'], X_valid[f+'_avg'], X_test[f+'_avg'] = target_encode(trn_series=X_train[f], \n",
        "                                                                               val_series=X_valid[f], \n",
        "                                                                               tst_series=X_test[f], \n",
        "                                                                               target=y_train, \n",
        "                                                                               min_samples_leaf=200, \n",
        "                                                                               smoothing=10, \n",
        "                                                                               noise_level=0\n",
        "                                                                               )\n",
        "        \n",
        "    # Run model for this fold\n",
        "    if OPTIMIZE_ROUNDS:\n",
        "        eval_set = [(X_valid, y_valid)]\n",
        "        fit_model = model.fit(X_train, \n",
        "                                y_train, \n",
        "                                eval_set=eval_set, \n",
        "                                eval_metric=gini_xgb, \n",
        "                                earyly_stopping_rounds=EARLY_STOPPING_ROUNDS, \n",
        "                                verbose=False\n",
        "                                )\n",
        "        print('Best N trees: ', model.best_ntree_limit)\n",
        "        print('Best gini: ', model.best_score)\n",
        "    else:\n",
        "        fit_model = model.fit(X_train, y_train)\n",
        "\n",
        "    # Generate validaton predictions for this fold\n",
        "    pred = fit_model.predict_proba(X_valid)[:, 1]\n",
        "    print('Gini: ', eval_gini(y_valid, pred))\n",
        "    y_valid_pred.iloc[test_index] = pred\n",
        "\n",
        "    # Accumulate test set predictions\n",
        "    y_test_pred += fit_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    del X_test, X_train, X_valid, y_train\n",
        "\n",
        "y_test_pred /= K # Average test set predictions\n",
        "\n",
        "print('\\nGini for full training set:')\n",
        "eval_gini(y, y_valid_pred)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-8c206a7ca99f>:1: NumbaWarning: \n",
            "Compilation is falling back to object mode WITH looplifting enabled because Function \"eval_gini\" failed type inference due to: non-precise type pyobject\n",
            "During: typing of argument at <ipython-input-4-8c206a7ca99f> (3)\n",
            "\n",
            "File \"<ipython-input-4-8c206a7ca99f>\", line 3:\n",
            "def eval_gini(y_true, y_prob):\n",
            "    y_true = np.asarray(y_true)\n",
            "    ^\n",
            "\n",
            "  @jit\n",
            "<ipython-input-4-8c206a7ca99f>:1: NumbaWarning: \n",
            "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"eval_gini\" failed type inference due to: cannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\n",
            "\n",
            "File \"<ipython-input-4-8c206a7ca99f>\", line 9:\n",
            "def eval_gini(y_true, y_prob):\n",
            "    <source elided>\n",
            "    n = len(y_true)\n",
            "    for i in range(n-1, -1, -1):\n",
            "    ^\n",
            "\n",
            "  @jit\n",
            "/usr/local/lib/python3.7/dist-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"eval_gini\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
            "\n",
            "File \"<ipython-input-4-8c206a7ca99f>\", line 3:\n",
            "def eval_gini(y_true, y_prob):\n",
            "    y_true = np.asarray(y_true)\n",
            "    ^\n",
            "\n",
            "  state.func_ir.loc))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
            "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
            "\n",
            "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
            "\n",
            "File \"<ipython-input-4-8c206a7ca99f>\", line 3:\n",
            "def eval_gini(y_true, y_prob):\n",
            "    y_true = np.asarray(y_true)\n",
            "    ^\n",
            "\n",
            "  state.func_ir.loc))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gini:  0.28569596122091456\n",
            "\n",
            "Fold 1\n",
            "Gini:  0.28252701829140636\n",
            "\n",
            "Fold 2\n",
            "Gini:  0.27441241547254536\n",
            "\n",
            "Fold 3\n",
            "Gini:  0.2992591316796187\n",
            "\n",
            "Fold 4\n",
            "Gini:  0.28468084622991796\n",
            "\n",
            "Gini for full training set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-8c206a7ca99f>:1: NumbaWarning: \n",
            "Compilation is falling back to object mode WITH looplifting enabled because Function \"eval_gini\" failed type inference due to: non-precise type pyobject\n",
            "During: typing of argument at <ipython-input-4-8c206a7ca99f> (3)\n",
            "\n",
            "File \"<ipython-input-4-8c206a7ca99f>\", line 3:\n",
            "def eval_gini(y_true, y_prob):\n",
            "    y_true = np.asarray(y_true)\n",
            "    ^\n",
            "\n",
            "  @jit\n",
            "<ipython-input-4-8c206a7ca99f>:1: NumbaWarning: \n",
            "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"eval_gini\" failed type inference due to: cannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\n",
            "\n",
            "File \"<ipython-input-4-8c206a7ca99f>\", line 9:\n",
            "def eval_gini(y_true, y_prob):\n",
            "    <source elided>\n",
            "    n = len(y_true)\n",
            "    for i in range(n-1, -1, -1):\n",
            "    ^\n",
            "\n",
            "  @jit\n",
            "/usr/local/lib/python3.7/dist-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"eval_gini\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
            "\n",
            "File \"<ipython-input-4-8c206a7ca99f>\", line 3:\n",
            "def eval_gini(y_true, y_prob):\n",
            "    y_true = np.asarray(y_true)\n",
            "    ^\n",
            "\n",
            "  state.func_ir.loc))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
            "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
            "\n",
            "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
            "\n",
            "File \"<ipython-input-4-8c206a7ca99f>\", line 3:\n",
            "def eval_gini(y_true, y_prob):\n",
            "    y_true = np.asarray(y_true)\n",
            "    ^\n",
            "\n",
            "  state.func_ir.loc))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2850918292886656"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxRyJ9cD8t27"
      },
      "source": [
        "# Save validation predictinos for stacking/ensembling\n",
        "val = pd.DataFrame()\n",
        "val['id'] = id_train\n",
        "val['target'] = y_valid_pred.values\n",
        "val.to_csv('xgb_valid.csv', float_format='%.6f', index=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOiPGB93qPYc"
      },
      "source": [
        "# Create submission file\n",
        "sub = pd.DataFrame()\n",
        "sub['id'] = id_test\n",
        "sub['target'] = y_test_pred\n",
        "sub.to_csv('xgb_submit.csv', float_format='%.6f', index=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFvs0zEhqd-H"
      },
      "source": [
        "* **version 16:** Baselin best CV=.2832, LB=.283\n",
        "* **version 15:** Ntree optimization for baseline\n",
        "* **version 21:** Verbose version of baseline optimization\n",
        "* **version 22:** Baselin + per-fold early stopping after 20 rounds\n",
        "* **version 23:** Back to baseline.\n",
        "* **version 24:** Some parameter tuning.\n",
        "* **versino 25:** Re-published to make it visible.\n",
        "* **version 26:** A little more tuning\n",
        "* **version 27:** More tuning, get rid of upsampling (using **_scale_pos_weight_** instead), _Set OPTIMIZE_ROUNDS_ and _verbose_ temporarily.\n",
        "* **version 28:** MAX_ROUNDS=300 as a compromise.\n",
        "* **version 29:** Substantively identical (Turn off now-irrelevant _verbose_)\n",
        "* **version 30:** Still substantively identical. Some visual cleanup.\n",
        "* **version 35:** More tuning. CV went up but LB sorts lower (still .283)\n",
        "* **version 36:** Identical (except turn off irrelevant _verbose_). Republished to make it visible.\n",
        "* **versions 37-42:** More tuning (min_child_weight=6). LB score has considerably improved according to sort, but still .284"
      ]
    }
  ]
}